[{"uri":"https://nt1510-l.github.io/aws-report.fcj/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Workshop Experience: \u0026ldquo;Building Intelligent Solutions with AWS AI/ML\u0026rdquo; Workshop Goals \u0026amp; Outcomes This comprehensive workshop focused on delivering practical knowledge across multiple areas:\nDeep dive into AWS\u0026rsquo;s complete AI/ML service portfolio Hands-on experience with Amazon SageMaker\u0026rsquo;s end-to-end platform Foundation Models exploration and implementation on Amazon Bedrock Advanced techniques in Prompt Engineering, RAG architecture, and intelligent agents Live construction of a production-ready GenAI application Session-by-Session Breakdown Opening Session (8:30 – 9:00 AM) | Foundation Setting Session Focus:\nWelcome reception and professional introductions Strategic objectives and learning roadmap Interactive team formation exercise Current state analysis of Vietnamese AI/ML adoption Learning Impact: Developed comprehensive understanding of AI/ML market dynamics in Vietnam and strategic positioning of AWS cloud services in enterprise digital transformation initiatives.\nCore Session (9:00 – 10:30 AM) | Machine Learning Platform Deep Dive Technical Coverage:\nComplete Amazon SageMaker ecosystem exploration Advanced data preprocessing and annotation workflows Model development, hyperparameter optimization, and production deployment Enterprise MLOps implementation and monitoring Interactive Demo: Complete SageMaker Studio environment tour Technical Mastery: Acquired proficiency in the complete ML development pipeline on AWS, including data preparation strategies, model lifecycle management, production deployment best practices, enterprise-grade MLOps implementation, and practical SageMaker Studio navigation.\nNetworking Break (10:30 – 10:45 AM) | Knowledge Exchange Structured networking session facilitating discussions with AWS solution architects and fellow practitioners.\nAdvanced Session (10:45 AM – 12:00 PM) | Generative AI Implementation Technology Stack:\nFoundation Model Selection: Claude, Llama, Titan — strategic comparison and optimization Advanced Prompt Engineering: Systematic techniques, Chain-of-Thought methodologies, Few-shot optimization RAG Implementation: System architecture design, Knowledge Base connectivity and optimization Bedrock Agents Development: Complex workflow automation, external tool integration Security Framework: Comprehensive guardrails implementation and content governance Practical Implementation: End-to-end GenAI chatbot development with Amazon Bedrock Advanced Capabilities: Mastered strategic Foundation Model selection criteria, advanced prompt engineering methodologies (CoT, few-shot optimization), complete RAG system architecture, Bedrock Agents for complex automation workflows, enterprise security and governance frameworks, and comprehensive GenAI application development lifecycle.\nStrategic Knowledge Acquisition Complete mastery of AWS ML and GenAI service ecosystem Strategic Foundation Model evaluation framework (Claude/Llama/Titan optimization) Advanced Chain-of-Thought and Few-shot learning implementation for enhanced output quality Complex prompt architecture design for enterprise-grade pipelines Enterprise RAG system necessity understanding and Bedrock Knowledge Base integration strategies Intelligent agent development with comprehensive tool ecosystem integration Production-grade SageMaker Studio expertise and complete ML workflow management Enterprise AI chatbot deployment using AWS security and compliance standards Professional Implementation Strategy Enterprise RAG integration for internal knowledge management and customer support automation SageMaker utilization for custom model training and fine-tuning initiatives Strategic Prompt Engineering deployment for Foundation Model output optimization Bedrock Agents integration for intelligent workflow automation GenAI proof-of-concept development for cross-functional team collaboration Professional Development Impact Participation in this intensive \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop delivered transformative insights into enterprise-scale AI/ML implementation methodologies and practical GenAI deployment strategies.\nWorkshop Excellence Factors Expert Knowledge Transfer: AWS solution architects provided strategic insights into AI/ML adoption roadmaps specifically tailored for Vietnamese enterprise environments, complemented by comprehensive live demonstrations of SageMaker and Bedrock platforms. Practical Implementation Sessions: Direct observation of complete train → optimize → deploy workflows, enhanced by detailed Bedrock chatbot development sessions that illuminated the entire GenAI application development lifecycle. Strategic Networking Opportunities: Structured interactions with AWS engineering teams and industry practitioners, facilitating knowledge exchange through real-world GenAI implementation case studies. Architectural Understanding: Recognition that GenAI represents a comprehensive technological ecosystem (Prompt Engineering → RAG Architecture → Agent Development → Security Guardrails), SageMaker provides standardized ML lifecycle management, and strategic model selection drives both operational efficiency and cost optimization. Workshop Documentation This comprehensive workshop experience transcended traditional technical training by fundamentally transforming my perspective on AI/ML implementation strategies, enterprise system modernization approaches, and collaborative development methodologies across multidisciplinary teams.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Meeting AWS members and admin - Enjoy AWS event - Find member and create group project 06/09/2025 06/09/2025 2 - Create new AWS account + View AWS account identifiers + Update the AWS account + Create or update your AWS account alias + Close a standalone AWS account - MFA for AWS Accounts + Virtual MFA Device +U2F Security Key + Hard MFA Device - Create Admin Group and Admin User - Account Authentication Support - Explore and Configure AWS Management Console + Set Up Default Region + Search with the AWS Management Console + Add and Remove Favorite AWS Services + Create and use dashboard widgets - Creating Support Cases and Case Management in AWS 07/09/2025 07/09/2025 https://000001.awsstudygroup.com/ 3 - Create Budget by Template - Create Cost Budget - Create Usage Budget - Create RI Budget - Create Savings Plans Budget - Clean Up Resources 08/09/2025 08/09/2025 https://000007.awsstudygroup.com/ 4 - AWS Support Packages - Access AWS Support + Types of Support Requests + Change support package - Manage Support Requests + Create Support Request + Select severity 10/09/2025 10/09/2025 https://000009.awsstudygroup.com/ 5 - Introduction + Subnets + Route Table + Internet Gateway + NAT Gateway - Firewall in VPC + Security Group + Network ACLs + VPC Resource Map - Preparation Steps + Create VPC + Create Subnet + Create Internet Gateway + Create Route Table + Create Security Group + Enable VPC Flow Logs - Deploying Amazon EC2 Instances + Create EC2 Server + Test Connection + Create NAT Gateway + Using Reachability Analyzer + Create EC2 Instance Connect Endpoint (Optional) + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting 11/09/2025 11/09/2025 https://000003.awsstudygroup.com/ 6 - Setting Up Site-to-Site VPN Connection in AWS + Create a VPN environment + Create VPC for VPN + Create EC2 as a Customer Gateway + Configure VPN Connection + Create Virtual Private Gateway + Create Customer Gateway + Create VPN Connection + Customer Gateway Configuration + Modify AWS VPN Tunnel + Alternative VPN Configurations + VPN Troubleshooting Guide + AWS Official VPN Troubleshooting Guide + VPN Connection using Strongswan with Transit Gateway (Optional) + Create Customer Gateway + Create Transit Gateway + Create VPN Connection + Create Transit Gateway Attachment + Configure Route Tables + Configure Customer Gateway - Clean up resources - Infrastructure as Code Templates 12/09/2025 12/09/2025 https://000003.awsstudygroup.com/ Week 1 Achievements Gained a clear understanding of AWS Cloud and its core service groups including Compute, Storage, Networking, and Database.\nSuccessfully created and configured an AWS Free Tier account, including multi-factor authentication (MFA) setup and IAM user management.\nBecame familiar with the AWS Management Console, learned how to search for services, customize the dashboard, and manage resources efficiently.\nInstalled and configured AWS CLI, and practiced basic commands to check account information and manage AWS resources.\nCreated and managed AWS Budgets, learned how to monitor usage and costs, and cleaned up unused resources.\nPracticed building a Virtual Private Cloud (VPC) with Subnets, Route Tables, Internet Gateway, NAT Gateway, and Security Groups; deployed and connected EC2 instances.\nExplored and configured VPN Site-to-Site connections, understanding how to link on-premises networks with AWS VPCs.\nImproved self-learning, troubleshooting, and cloud management skills through hands-on AWS exercises.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Thanh Liem\nPhone Number: 0903926602\nEmail: liemntse184163@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will find a comprehensive 12-week AWS learning program designed to provide hands-on experience with enterprise-level cloud technologies. This worklog documents a structured journey from AWS fundamentals to advanced data analytics and serverless architectures.\nProgram Overview This intensive 3-month program covers the complete AWS ecosystem through practical labs and real-world scenarios. Each week builds upon previous knowledge, progressing from basic services to sophisticated cloud solutions including storage, compute, networking, databases, security, analytics, and modern application architectures.\nLearning Path \u0026amp; Weekly Breakdown Week 1: AWS Fundamentals \u0026amp; Cloud Introduction\nGetting familiar with AWS and basic cloud concepts AWS account setup and initial service exploration Week 2: Core AWS Services Deep Dive\nEC2, S3, VPC fundamentals and hands-on labs Basic networking and storage configurations Week 3: Advanced Infrastructure \u0026amp; Security\nAdvanced networking, security groups, and IAM Infrastructure as Code introduction Week 4: Storage Solutions \u0026amp; Website Hosting\nLab 13: S3 Static Websites \u0026amp; Lab 14: VM Import/Export Storage optimization and content delivery Week 5: Enterprise Storage \u0026amp; File Systems\nLab 24: Storage Gateway \u0026amp; Lab 25: File Systems Hybrid cloud storage solutions Week 6: CDN \u0026amp; Security Monitoring\nLab 57: S3 + CloudFront \u0026amp; Lab 18: Security Hub Content delivery and security best practices Week 7: Serverless \u0026amp; Advanced IAM\nLab 22: VPC + EC2 + Lambda, Lab 27: Tags, Lab 28: IAM Serverless architectures and access management Week 8: Enterprise Security \u0026amp; Compliance\nLab 30: IAM Restrictions, Lab 33: KMS + CloudTrail + Athena, Lab 44: Advanced IAM Security hardening and audit logging Week 9: Infrastructure \u0026amp; Data Migration\nLab 48: Multi-service Integration, Lab 05: VPC + RDS, Lab 43: DMS, Lab 35: Data Analytics Database migration and analytics foundations Week 10: Advanced NoSQL \u0026amp; Data Engineering\nLab 39: DynamoDB Comprehensive Training Advanced design patterns, AI integration, serverless applications, and real-world scenarios Week 11: Data Analytics Ecosystem\nLab 40: AWS Glue \u0026amp; Athena, Lab 60: DynamoDB Fundamentals, Lab 70: DataBrew Complete data pipeline construction and management Week 12: Advanced Analytics \u0026amp; Business Intelligence\nLab 72: Complete Analytics Pipeline, Lab 73: Advanced QuickSight Dashboards Enterprise BI solutions and interactive visualization Key Learning Outcomes By completing this program, participants will have gained:\nFoundational Knowledge: AWS core services, cloud architecture principles Technical Skills: Hands-on experience with 17+ AWS services across compute, storage, database, analytics, and security Advanced Expertise: NoSQL design patterns, data engineering pipelines, serverless architectures Business Intelligence: Professional dashboard development and data visualization Enterprise Readiness: Security best practices, cost optimization, and production deployment strategies "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Amazon QuickSight Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. QuickSight connects to your data in the cloud and combines data from many different sources. In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more. Key Features \u0026ldquo;SPICE: \u0026ldquo; The Super-fast, Parallel, In-memory Calculation Engine. SPICE is engineered to rapidly perform advanced calculations and serve data. \u0026ldquo;Auto-Graph\u0026rdquo; Automatically selects the best visualization for your data. \u0026ldquo;ML Insights: \u0026ldquo; Leverages machine learning to uncover hidden trends and outliers. \u0026ldquo;Embedded Analytics: \u0026ldquo; Embed dashboards into your applications. Workshop Scenario In this workshop, you will act as a Data Analyst for a retail company. You have been given a dataset of sales records and tasked with creating a dashboard to visualize sales performance by region, date, and product category.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Document QA with AWS Bedrock Intelligent Document Analysis System using RAG 1. Executive Summary The Document QA system is a serverless application designed to revolutionize how users interact with documents. By leveraging AWS Bedrock for Generative AI and RAG (Retrieval Augmented Generation) technology, the platform allows users to upload PDF/TXT documents and ask natural language questions. The system provides accurate, context-aware answers by retrieving relevant information from the uploaded documents, significantly reducing manual search time and improving information accessibility.\n2. Problem Statement What’s the Problem? Traditional document search methods (keyword matching) often fail to capture context or semantic meaning. Manual document review is time-consuming, error-prone, and inefficient, especially for large volumes of text. Users struggle to extract specific insights quickly, leading to productivity bottlenecks.\nThe Solution We propose a Serverless RAG-based Chatbot using AWS Bedrock (Amazon Titan). The solution involves:\nUpload \u0026amp; Processing: Users upload documents to S3; Lambda functions trigger text extraction and embedding generation. Vector Search: Embeddings are stored and queried to find relevant document chunks. Generative AI: AWS Bedrock generates natural language responses based on the retrieved context. Serverless Architecture: Built on AWS Lambda, API Gateway, and DynamoDB for automatic scaling and cost efficiency. Benefits and Return on Investment Efficiency: Reduces document analysis time from hours to seconds. Accuracy: RAG ensures answers are grounded in the provided document, minimizing hallucinations. Cost-Effective: Serverless pay-as-you-go model (estimated \u0026lt; $5/month for low usage). Scalability: Automatically handles varying loads without manual infrastructure management. 3. Solution Architecture The platform employs a modern serverless architecture to ensure scalability, security, and performance.\nAWS Services Used AWS Bedrock: Provides the Foundation Models (Amazon Titan) for embeddings and text generation. AWS Lambda: Serverless compute for handling API requests, document processing, and orchestration. Amazon API Gateway: Manages REST API endpoints for the frontend. Amazon S3: Stores raw uploaded documents and frontend static assets. Amazon DynamoDB: Manages user sessions and chat history. Vector Store: (Implemented via Lambda/Local or dedicated vector DB) Stores document embeddings for semantic search. Component Design Frontend: Hosted on S3 (or Amplify), providing a user-friendly chat interface. API Layer: API Gateway routes requests (/upload, /ask) to Lambda functions. Processing Layer: Lambda handles text extraction, calls Bedrock for embeddings, and performs vector similarity search. AI Layer: AWS Bedrock generates responses using the retrieved context and user query. 4. Technical Implementation Implementation Phases\nPhase 1: Foundation (Weeks 1-4): Setup AWS environment, Bedrock access, and basic backend logic. Phase 2: API \u0026amp; Security (Weeks 5-7): Develop API Gateway, Lambda functions, and implement CORS/Security. Phase 3: Frontend Development (Weeks 8-11): Build the React/Next.js interface and integrate with APIs. Phase 4: Testing \u0026amp; Deployment (Weeks 12-14): End-to-end testing, optimization, and final deployment. Technical Requirements\nAI Model: Amazon Titan (via Bedrock) for Embeddings and Text Generation. Backend: Node.js/Python on AWS Lambda. Infrastructure as Code: Serverless Framework or AWS CDK. Frontend: React.js / Next.js. 5. Timeline \u0026amp; Milestones Month 1: Architecture Design, AWS Setup, Backend Core (Upload/Embeddings). Month 2: RAG Implementation, Vector Search Logic, API Development. Month 3: Frontend Integration, UI/UX Polish, Testing, and Launch. 6. Budget Estimation Estimated Monthly Costs (Low-Medium Usage)\nAWS Bedrock (Titan): ~$0 (Free Tier / Low cost per 1k tokens) AWS Lambda: ~$0.20 per 1M requests Amazon S3: ~$0.023 per GB Amazon DynamoDB: ~$0.25 per 1M requests Amazon API Gateway: ~$3.50 per 1M requests Total Estimated: \u0026lt; $5.00 / month\n7. Risk Assessment Risk Matrix Hallucinations (AI Errors): Medium Impact, Medium Probability. Cost Overruns: Medium Impact, Low Probability (Serverless). Data Leakage: High Impact, Low Probability. Mitigation Strategies Hallucinations: Strict RAG implementation (grounding answers in context). Cost: Set AWS Budget Alerts and usage quotas. Security: Use Presigned URLs for S3, IAM roles with least privilege. 8. Expected Outcomes Technical Improvements Fully automated document analysis pipeline. Sub-second retrieval latency for vector search. Scalable architecture supporting concurrent users. Long-term Value A reusable RAG framework for future knowledge base applications. Significant productivity gains for users needing quick information retrieval. "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Professional Development Session: \u0026ldquo;Advanced DevOps Implementation with AWS Cloud Services\u0026rdquo; Learning Objectives \u0026amp; Focus Areas Master DevOps methodologies and cultural transformation in cloud-native environments Architect comprehensive CI/CD automation using AWS DevOps toolkit Achieve expertise in Infrastructure as Code through CloudFormation and CDK Explore containerization ecosystem on AWS (ECR, ECS, EKS, App Runner) Deploy comprehensive monitoring and observability frameworks using CloudWatch and X-Ray Integrate DevOps excellence patterns and analyze industry implementation cases Session Breakdown \u0026amp; Technical Coverage 8:30 – 9:00 | Foundation Setting \u0026amp; Cultural Framework Session Activities:\nProfessional welcome and AI/ML workshop continuation DevOps cultural paradigms and organizational principles Performance indicators and success metrics (DORA, MTTR, deployment velocity) Strategic role of DevOps in enterprise digital evolution Technical Mastery: Recognized DevOps as a comprehensive cultural transformation beyond tooling, acquired proficiency in quantitative DevOps assessment metrics and their strategic implementation in organizational contexts.\n9:00 – 10:30 | DevOps Automation Architecture – Complete Pipeline Engineering Technical Deep Dive:\nVersion Control Systems: AWS CodeCommit implementation and Git workflow methodologies (GitFlow, Trunk-based development) Automated Build Systems: CodeBuild configuration, comprehensive testing automation Strategic Deployment: CodeDeploy with Blue/Green, Canary, and Rolling deployment patterns Pipeline Orchestration: Advanced automation through CodePipeline Practical Implementation: End-to-end CI/CD pipeline demonstration Advanced Proficiency: Achieved mastery in complete CI/CD pipeline architecture from source management through production deployment, comprehensive understanding of deployment strategy selection, and extensive practical experience with AWS DevOps service ecosystem.\n10:30 – 10:45 | Professional Networking Strategic discussions and knowledge exchange with AWS solution architects and fellow practitioners.\n10:45 – 12:00 | Infrastructure Automation \u0026amp; Code Management Technical Framework:\nAWS CloudFormation: Advanced template design, stack management, and configuration drift monitoring AWS CDK (Cloud Development Kit): Programmatic constructs, reusable architectural patterns, multi-language support Hands-on Implementation: Live deployment demonstrations using CloudFormation and CDK Strategic Analysis: Decision framework for optimal IaC tool selection Infrastructure Expertise: Developed comprehensive Infrastructure as Code proficiency, mastered comparative analysis between CloudFormation and CDK approaches, and learned systematic application of IaC for efficient, repeatable infrastructure management.\n12:00 – 13:00 | Extended Break (Individual Arrangement) 13:00 – 14:30 | Containerization \u0026amp; Orchestration Ecosystem Platform Exploration:\nContainer Fundamentals: Microservices architecture and containerization strategies Amazon ECR: Container registry management, vulnerability scanning, lifecycle automation Amazon ECS \u0026amp; EKS: Advanced deployment methodologies, auto-scaling, orchestration frameworks AWS App Runner: Streamlined container deployment solutions Comparative Analysis: Microservices deployment strategy evaluation Container Mastery: Acquired comprehensive containerization knowledge, understood strategic differences between ECS and EKS platforms, mastered container image lifecycle management through ECR, and gained practical microservices deployment experience.\n14:30 – 14:45 | Technical Break 14:45 – 16:00 | Observability \u0026amp; Performance Intelligence Monitoring Framework:\nCloudWatch: Advanced metrics collection, log aggregation, intelligent alerting, dashboard visualization AWS X-Ray: Distributed system tracing and performance analytics Implementation Workshop: Comprehensive observability architecture setup Excellence Patterns: Advanced alerting strategies, dashboard design, incident response workflows Monitoring Expertise: Mastered effective monitoring and observability implementation, gained proficiency in CloudWatch and X-Ray for comprehensive application monitoring and debugging, and applied professional best practices in alerting and incident management.\n16:00 – 16:45 | DevOps Excellence Patterns \u0026amp; Industry Case Analysis Advanced Methodologies:\nStrategic Deployment Approaches: Feature flag management, A/B testing frameworks Quality Assurance Integration: Automated testing pipeline integration Incident Response: Professional incident management and post-incident analysis Industry Studies: DevOps transformation case studies from startup to enterprise scale Strategic Implementation: Acquired advanced deployment strategy expertise, learned systematic automated testing integration, mastered professional incident management processes, and gained practical insights from comprehensive industry transformation case studies.\n16:45 – 17:00 | Professional Development \u0026amp; Session Conclusion Career Focus:\nDevOps career progression pathways and industry opportunities AWS certification strategy for DevOps Engineering professionals Interactive discussion and comprehensive Q\u0026amp;A session Strategic Knowledge Acquisition Comprehensive DevOps cultural understanding and practical AWS implementation Advanced CI/CD pipeline construction using AWS CodeCommit, CodeBuild, CodeDeploy, CodePipeline Infrastructure as Code mastery through CloudFormation and CDK frameworks Comprehensive containerization knowledge: ECR, ECS, EKS, App Runner ecosystems Advanced monitoring and observability implementation using CloudWatch and X-Ray DevOps excellence integration: deployment strategies, automated testing, incident management protocols Industry-validated experience through comprehensive case studies and practical demonstrations Professional Implementation Strategies Architect CI/CD automation for existing project portfolios Deploy Infrastructure as Code for systematic infrastructure management Implement containerization strategies for microservices architectural transformation Establish comprehensive monitoring and intelligent alerting ecosystems Integrate advanced deployment strategies including Blue/Green and Canary deployment patterns Enhance incident management protocols and post-incident analysis processes Professional Development Experience Participation in \u0026ldquo;Advanced DevOps Implementation with AWS Cloud Services\u0026rdquo; delivered exceptional value, providing comprehensive understanding of DevOps practice implementation within AWS cloud environments.\nExcellence Factors Expert Knowledge Transfer: Direct access to experienced AWS Solution Architects, gaining comprehensive DevOps mindset understanding and practical implementation strategies. Comprehensive Technical Demonstrations: Direct observation of complete CI/CD pipeline construction processes, Infrastructure as Code implementations using CloudFormation and CDK. Industry Case Study Analysis: Learning from successful DevOps implementation transformations across startup and enterprise environments. Strategic Professional Networking: Opportunities for structured discussions with DevOps Engineers and Cloud Architects, facilitating experience sharing and workplace challenge resolution. Strategic Learning Outcomes DevOps represents a continuous improvement journey requiring systematic implementation and ongoing optimization Infrastructure as Code provides foundational framework for scalable DevOps implementation Monitoring and observability serve as critical components for maintaining system reliability and performance Container technology represents the evolutionary future of application deployment and orchestration Technical Documentation This comprehensive professional development experience transcended traditional technical training by providing fundamental understanding of DevOps cultural importance in enhancing Development and Operations team collaboration, thereby significantly improving product quality and delivery velocity.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction - Preparation + Create Key Pair + Initialize CloudFormation Template + Configure Security Group - Connect to RDGW - Deploy Microsoft AD - Set up DNS + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results - Clean up resources 15/09/2025 15/09/2025 \u0026lt; https://000010.awsstudygroup.com/\u003e 3 - Introduction - Prerequisites + Initialize CloudFormation Template + Create Security Group + Create EC2 Instance - Update Network ACL - VPC Peering - Route Tables - Cross-Peer DNS - Clean up resources 16/09/2025 16/09/2025 \u0026lt; https://000019.awsstudygroup.com/\u003e 4 - Introduction - Preparation + Create Key Pair + Initialize CloudFormation Template - Create Transit Gateway - Create Transit Gateway Attachments - Create Transit Gateway Route Tables - Add Transit Gateway Routes to VPC Route Tables - Clean up resources 17/09/2025 17/09/2025 \u0026lt; https://000020.awsstudygroup.com/\u003e 5 - Introduction - Preparation + Create S3 Bucket + Deploy infrastructure - Create Backup plan - Set up notifications - Test Restore - Clean up resources 18/09/2025 18/09/2025 https://000013.awsstudygroup.com/\u003e 6 - Preparation + Create S3 Bucket + Create EC2 for Storage Gateway - Using AWS Storage Gateway + Create Storage Gateway + Create File Shares + Mount File Shares on On-premises machine - Clean up resources 19/09/2025 19/09/2025 https://000024.awsstudygroup.com/ Week 2 Achievements Understood and deployed Microsoft Active Directory (AD) on AWS, including creating Key Pair, CloudFormation Stack, and configuring Security Group.\nSet up internal DNS with Amazon Route 53, created and tested Outbound Endpoint, Resolver Rules, and Inbound Endpoints to ensure accurate domain name resolution between networks.\nPracticed VPC Peering between VPC networks, updated Network ACL and Route Tables to enable secure cross-region connectivity.\nConfigured DNS Resolution between peered VPCs, ensuring servers can communicate seamlessly via internal domain names.\nCreated and managed AWS Transit Gateway, configured Attachments and Route Tables to connect multiple VPCs in a central network system.\nDeployed and tested AWS Backup, created Backup Plan, set up notifications, and successfully performed Test Restore.\nBecame familiar with AWS Storage Gateway, configured File Gateway, created File Share, and successfully connected to on-premises machine.\nPracticed skills in deploying, monitoring, and cleaning up AWS infrastructure resources professionally.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.2-prerequiste/","title":"Prerequisites &amp; Data","tags":[],"description":"","content":"Step 1: Sign up for Amazon QuickSight Log in to the AWS Management Console. Search for QuickSight. If you have not signed up yet, you will be prompted to do so. Choose Sign up for QuickSight. Select the Enterprise edition (it offers a free trial). Follow the on-screen instructions: Authentication method: Use IAM federated identity \u0026amp; QuickSight-managed users (default). Region: Select US East (N. Virginia). QuickSight account name: Enter a unique name. Notification email: Enter your email. Allow access: You can leave the defaults or uncheck them if we are only uploading a file. For this workshop, we don\u0026rsquo;t strictly need S3 access, but it\u0026rsquo;s good practice to leave S3 checked if you plan to use it later. Click Finish. Step 2: Prepare Sample Data We will use a simple CSV file for this workshop.\nCopy the following data and save it as a file named sales_data.csv on your computer: Date,Region,Product,Sales,Quantity 2023-01-01,North,Laptop,1200,2 2023-01-02,South,Phone,800,5 2023-01-03,East,Tablet,400,3 2023-01-04,West,Laptop,1200,1 2023-01-05,North,Phone,800,2 2023-01-06,South,Tablet,400,4 2023-01-07,East,Laptop,1200,3 2023-01-08,West,Phone,800,6 2023-01-09,North,Tablet,400,2 2023-01-10,South,Laptop,1200,4 Alternatively, you can use any CSV file you have, but the instructions will follow this structure.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Professional Workshop Experience: \u0026ldquo;Advanced Security Architecture - AWS Well-Architected Security Framework Implementation\u0026rdquo; Workshop Goals \u0026amp; Learning Outcomes Achieve comprehensive mastery of AWS Well-Architected Security Framework\u0026rsquo;s 5 foundational pillars Develop expertise in contemporary security threat mitigation and cloud security excellence practices Acquire advanced skills in end-to-end security implementation spanning identity governance through incident response protocols Deploy security architectural principles across complex enterprise production environments Technical Session Architecture \u0026amp; Core Learnings 8:30 – 8:50 AM | Foundation Establishment \u0026amp; Security Paradigms Technical Framework:\nSecurity Pillar integration within comprehensive Well-Architected methodology Foundational security principles: Least Privilege enforcement – Zero Trust implementation – Defense in Depth strategy Comprehensive Shared Responsibility Model application across service tiers Critical cloud security threat analysis within Vietnamese enterprise ecosystems Advanced Mastery: Developed comprehensive understanding of AWS security architectural philosophy and practical application of shared responsibility across diverse service categories. Acquired detailed knowledge of contemporary security threat landscape and prevalent vulnerability patterns in Vietnamese cloud infrastructure deployments.\n8:50 – 9:30 AM | Security Pillar 1 — Advanced Identity \u0026amp; Access Governance Comprehensive Coverage:\nContemporary IAM Architecture: Advanced user management, role-based access, policy optimization strategies Strategic elimination of persistent credentials through temporary access token implementation IAM Identity Center: Enterprise SSO deployment and granular permission set configuration Service Control Policies \u0026amp; permission boundaries for sophisticated multi-account organizational governance Multi-factor authentication deployment, automated credential rotation methodologies, Access Analyzer implementation Practical Workshop: Advanced IAM policy validation and comprehensive access simulation Technical Proficiency: Achieved mastery in contemporary IAM architectural patterns, recognized critical significance of persistent credential elimination, and developed extensive hands-on expertise with advanced policy validation frameworks. Acquired comprehensive multi-account security governance implementation strategies.\n9:30 – 9:55 AM | Security Pillar 2 — Advanced Detection \u0026amp; Continuous Monitoring Architecture Technical Implementation:\nCloudTrail enterprise-scale logging architecture and organizational management strategies GuardDuty intelligent threat detection integration with Security Hub centralized dashboard Comprehensive logging ecosystem: VPC Flow Logs, Application Load Balancer logs, S3 access pattern analysis Advanced alerting automation through EventBridge architectural integration Detection-as-Code methodologies for infrastructure monitoring and security rule governance Strategic Expertise: Developed comprehensive monitoring strategy implementation across complete AWS service architecture, acquired advanced skills in automated threat detection and response orchestration, and gained strategic insight into codified security rule management for enhanced organizational governance.\n9:55 – 10:10 AM | Professional Networking Break Strategic discussions and knowledge exchange with security specialists and AWS solution architects.\n10:10 – 10:40 AM | Security Pillar 3 — Comprehensive Infrastructure Protection Advanced Architecture:\nVPC network segmentation strategies and sophisticated isolation methodologies Strategic subnet placement: private versus public deployment optimization patterns Security Groups versus Network ACLs: comprehensive application modeling and implementation strategies Integrated protection framework: WAF + Shield + Network Firewall architectural convergence Advanced workload protection: EC2 security hardening, ECS/EKS container security architecture Infrastructure Mastery: Achieved mastery in layered network security architectural approaches, developed expertise in strategic network security control selection and deployment, and acquired comprehensive workload-specific security implementation strategies for containerized and traditional compute environments.\n10:40 – 11:10 AM | Security Pillar 4 — Advanced Data Protection \u0026amp; Encryption Architecture Comprehensive Security Framework:\nKMS: Advanced key policy design, granular grants management, automated rotation orchestration Comprehensive encryption implementation: at-rest \u0026amp; in-transit across S3, EBS, RDS, DynamoDB service ecosystems Secrets Manager \u0026amp; Parameter Store: Advanced rotation pattern implementation and lifecycle management Enterprise data classification frameworks and sophisticated access control guardrails Data Security Expertise: Developed comprehensive understanding of AWS encryption service architecture, acquired advanced patterns for secrets lifecycle management and automated rotation strategies, and mastered implementation of data classification and protection methodologies across diverse service architectures.\n11:10 – 11:40 AM | Security Pillar 5 — Strategic Incident Response \u0026amp; Security Orchestration Advanced Response Architecture:\nComprehensive incident response lifecycle implementation following AWS security excellence practices Advanced response playbooks for critical security scenarios: Compromised IAM credential forensics and remediation S3 bucket exposure incidents and rapid containment EC2 malware detection, analysis, and systematic response protocols Advanced forensic procedures: snapshot creation, workload isolation, evidence preservation and collection Automated incident response orchestration using Lambda functions and Step Functions workflow automation Incident Response Mastery: Acquired advanced incident response capabilities using AWS-native security tools, developed expertise in automated response pattern implementation, and mastered security orchestration frameworks for accelerated incident resolution and comprehensive threat mitigation.\n11:40 – 12:00 PM | Strategic Synthesis \u0026amp; Professional Development Comprehensive Integration:\nStrategic synthesis and integration review across all 5 security architectural pillars Critical implementation challenges and practical solutions for Vietnamese enterprise environments Professional security advancement roadmap: Security Specialty certification and Solutions Architect Professional pathways Strategic Knowledge Acquisition Advanced Security Architecture: Comprehensive mastery of all 5 Well-Architected Security Pillars with deep understanding of architectural interconnections and dependencies Contemporary IAM Excellence: Advanced identity and access governance implementation with comprehensive zero-trust architectural principles Detection \u0026amp; Response Orchestration: Implementation of sophisticated monitoring ecosystems and automated incident response frameworks Infrastructure Security Mastery: Advanced network segmentation strategies and comprehensive workload protection methodologies Data Protection Excellence: Advanced encryption frameworks and comprehensive secrets lifecycle management strategies Enterprise Implementation: Real-world security scenario analysis and enterprise-specific challenge resolution strategies Professional Implementation Strategies Deploy comprehensive security architecture assessment utilizing Well-Architected Security Pillar evaluation framework Execute complete IAM architecture transformation following contemporary excellence practices and systematic long-term credential elimination Establish automated threat detection and response orchestration through GuardDuty, Security Hub, and EventBridge integration Deploy advanced network segmentation and comprehensive workload protection across existing infrastructure environments Implement enterprise data classification frameworks and encryption standardization across all service ecosystems Develop comprehensive incident response automation and playbook frameworks for critical security scenario management Professional Development Impact Engagement in \u0026ldquo;Advanced Security Architecture - AWS Well-Architected Security Framework Implementation\u0026rdquo; delivered exceptional value through comprehensive cloud security excellence education and sophisticated implementation methodologies.\nExcellence Factors Specialist-Led Security Architecture Training: Advanced learning from AWS security architects with extensive enterprise implementation expertise Interactive Technical Demonstrations: Comprehensive practical workshops covering IAM policy validation, threat detection orchestration, and incident response automation Enterprise Case Study Analysis: In-depth analysis of actual security incident scenarios and comprehensive response strategy implementations Complete Architectural Coverage: Comprehensive exploration of all 5 security pillars with detailed implementation guidance and best practice frameworks Strategic Learning Outcomes Security represents a continuous improvement lifecycle requiring persistent monitoring, evaluation, and architectural enhancement rather than single-point implementation Zero-trust architectural paradigms demand fundamental transformation in identity and access management approaches and organizational security culture Automation serves as the cornerstone for effective enterprise-scale security, particularly critical for detection orchestration and incident response workflows Comprehensive data protection requires implementation across all architectural layers, from foundational infrastructure through application-specific security controls Workshop Documentation This comprehensive security-focused professional development experience substantially advanced my understanding of cloud security architecture and delivered practical frameworks for implementing enterprise-grade security across AWS environments. The systematic coverage of all security architectural pillars combined with extensive hands-on methodology transformed complex security concepts into accessible and immediately deployable solutions.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Preparation + Create S3 Bucket + Load data - Enable static website feature - Configure public access block - Configure public objects - Test website 09/22/2025 09/22/2025 \u0026lt; https://000057.awsstudygroup.com/\u003e 3 - Preparation + Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; - Accelerate Static Websites with CloudFront + Block all public access + Configure Amazon CloudFront + Test Amazon CloudFront 09/23/2025 09/23/2025 https://000057.awsstudygroup.com/ 4 - Bucket Versioning + Move objects + Replication (multi-Region) - Clean up resources 09 /24/2025 09/24/2025 https://000057.awsstudygroup.com/ 5 - **Practice:**Launch and Manage an EC2 Instance with EBS Storage - Deploy an EC2 Instance, Connect via SSH, and Attach an EBS Volume 09/24/2025 09/24/2025 https://000057.awsstudygroup.com/ 6 - Practice: Hands-on Lab: EC2 Instance Deployment and EBS Volume management 09/25/2025 09/25/2025 https://000057.awsstudygroup.com/ Week 3 Achievements: Gained hands-on experience with S3 Static Website Hosting, including:\nCreating and configuring an S3 bucketCompute Uploading website data Enabling static website hosting Configuring Public Access Block settings Setting public read permissions for objects Learned how to accelerate content using Amazon CloudFront:\nBlocking all public access at the S3 layer Creating and configuring a CloudFront distribution Testing content delivery through the CloudFront URL Practiced advanced S3 management features, including:\nEnabling Bucket Versioning Moving and organizing objects Setting up Multi-Region Object Replication Completed a full EC2 deployment workflow, including:\nLaunching an EC2 instance with proper configurations Selecting AMI, instance type, key pair, and networking Verifying system access and performing basic Linux operations Improved understanding of how compute and storage services integrate:\nEC2 and EBS interaction within the same AZ Practical workflows for real-world system administration "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in three important events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: Comprehensive workshop on AI/ML/GenAI on AWS, including Amazon SageMaker, Foundation Models on Bedrock, Prompt Engineering, RAG, and building GenAI chatbots.\nValue Gained: Deep understanding of AWS AI/ML ecosystem, practical skills with SageMaker and Bedrock, ability to apply AI/GenAI to real-world projects.\nEvent 2 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: Event focused on DevOps practices on AWS, including CI/CD pipelines, Infrastructure as Code, Container services, and Monitoring \u0026amp; Observability.\nValue Gained: Mastery of DevOps culture and implementation on AWS, proficiency in CI/CD with CodeCommit/CodeBuild/CodeDeploy/CodePipeline, deep understanding of IaC and container orchestration.\nEvent 3 Event Name: AWS Cloud Mastery Series #3 - Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: In-depth workshop on the 5 security pillars of AWS Well-Architected Framework: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nValue Gained: Comprehensive understanding of cloud security architecture, mastery of modern IAM practices with zero-trust principles, skills to implement comprehensive monitoring and automated incident response.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Preparation + Create S3 Bucket + Load data - Enable static website feature - Configure public access block - Configure public objects - Test website 09/29/2025 09/29/2025 \u0026lt; https://000013.awsstudygroup.com/\u003e 3 - Set up notifications - Test Restore - Clean up resources 09/30/2025 09/30/2025 \u0026lt; https://000013.awsstudygroup.com/\u003e 4 - VMWare Workstation - Import virtual machine to AWS + Export Virtual Machine from On-premises + Upload virtual machine to AWS + Import virtual machine to AWS + Deploy Instance from AMI 10/01/2025 10/01/2025 \u0026lt; https://000014.awsstudygroup.com/\u003e 5 - Export instance from AWS + Setting up S3 bucket ACL + Export virtual machine from Instance + Export virtual machine from AMI - Reference video - Resource Cleanup on AWS 10/02/2025 10/02/2025 \u0026lt; https://000014.awsstudygroup.com/\u003e 6 - Review and practice week\u0026rsquo;s activities + Review S3 static website setup + Practice VM import/export process + Review backup and restore procedures - Consolidate knowledge and troubleshoot issues 10/03/2025 10/03/2025 Week 4 Achievements: Successfully created and configured S3 buckets for static website hosting:\nConfigured bucket policies for public access Uploaded website files and assets Enabled static website hosting feature Configured public access blocks appropriately Tested website functionality and accessibility Mastered S3 backup and restore operations:\nSet up backup notifications Performed backup testing procedures Successfully restored data from backups Implemented proper resource cleanup procedures Gained hands-on experience with VM migration to AWS:\nSet up VMware Workstation environment Exported virtual machines from on-premises infrastructure Uploaded VM files to AWS S3 storage Successfully imported VMs using AWS VM Import/Export service Deployed EC2 instances from imported AMIs Learned reverse migration process from AWS:\nConfigured S3 bucket ACLs for VM export Exported running instances back to VM format Exported AMIs to downloadable VM files Performed complete resource cleanup on AWS Developed proficiency in AWS storage and compute services:\nS3 bucket management and configuration EC2 instance deployment and management AMI creation and manipulation Cross-platform VM migration workflows Acquired practical skills in cloud migration strategies and backup/restore procedures essential for enterprise environments.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Preparation + Create S3 Bucket + Create EC2 for Storage Gateway 10/06/2025 10/06/2025 \u0026lt; https://000024.awsstudygroup.com/\u003e 3 - Using AWS Storage Gateway + Create Storage Gateway + Create File Shares + Mount File shares on On-premises machine - Clean up resources 10/07/2025 10/07/2025 \u0026lt; https://000024.awsstudygroup.com/\u003e 4 - Introduction - Preparation + Create Environment + Create an SSD Multi-AZ file system + Create an HDD Multi-AZ file system - Create new file shares 10/08/2025 10/08/2025 \u0026lt; https://000025.awsstudygroup.com/\u003e 5 - Test Performance - Monitor Performance - Enable data deduplication - Enable shadow copies - Manage user sessions and open files - Enable user storage quotas - Enable Continuous Access share 10/09/2025 10/09/2025 \u0026lt; https://000025.awsstudygroup.com/\u003e 6 - Scale throughput capacity - Scale storage capacity - Delete environment - Using the AWS CLI (reference) 10/10/2025 10/10/2025 \u0026lt; https://000025.awsstudygroup.com/\u003e Week 5 Achievements: Successfully implemented AWS Storage Gateway solutions:\nCreated and configured S3 buckets for storage gateway backend Set up EC2 instances as storage gateway appliances Deployed and configured AWS Storage Gateway service Created and managed file shares for hybrid cloud storage Successfully mounted file shares on on-premises machines Performed complete resource cleanup procedures Mastered AWS file system management and operations:\nCreated comprehensive test environments for file systems Deployed both SSD and HDD Multi-AZ file systems Configured and managed new file shares across different storage types Conducted performance testing and monitoring of file systems Implemented advanced features including data deduplication and shadow copies Gained expertise in enterprise storage management features:\nManaged user sessions and monitored open files Configured user storage quotas for resource management Enabled Continuous Access shares for high availability Successfully scaled throughput and storage capacity dynamically Performed controlled environment deletion and cleanup Developed proficiency in AWS CLI operations for storage services:\nUsed AWS CLI for storage gateway management commands Executed file system operations via command line interface Automated storage scaling and configuration tasks Implemented monitoring and management scripts Acquired hands-on experience with hybrid cloud storage architectures:\nIntegrated on-premises systems with AWS cloud storage Configured seamless data flow between local and cloud environments Implemented enterprise-grade storage solutions with AWS services Optimized storage performance and cost efficiency across hybrid infrastructure Mastered advanced storage features essential for enterprise environments including backup strategies, performance optimization, and scalable storage architectures.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.5-publish-dashboard/","title":"Publish Dashboard","tags":[],"description":"","content":"PUBLISH DASHBOARD Step 1: Arrange Visuals You can resize and move the visuals on the sheet to make them look organized. Click on the title of each visual to rename it (e.g., \u0026ldquo;Sales by Product\u0026rdquo;). Add a title to the sheet at the top (e.g., \u0026ldquo;Sales Overview Dashboard\u0026rdquo;). Step 2: Publish Dashboard At the top right, choose Share \u0026gt; Publish dashboard. Publish new dashboard as: Enter a name, e.g., Sales-Dashboard-v1. Choose Publish dashboard. You can now share this dashboard with other users in your QuickSight account or share it via a link. Congratulations! You have successfully created a Business Intelligence dashboard using Amazon QuickSight.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/","title":"Workshop&#34;","tags":[],"description":"","content":"Overview In this workshop, you will learn how to use Amazon QuickSight, a fast, cloud-powered Business Intelligence (BI) service that makes it easy to deliver insights to everyone in your organization.\nYou will sign up for Amazon QuickSight, connect to data sources, create analyses with various visualizations, and finally publish a dashboard.\nContent Workshop overview Prerequisites \u0026amp; Data Preparation Connect to Data Create Visualizations Publish Dashboard Clean up resources "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Internship Process Evaluation Throughout my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 09/09/2025 to 13/12/2025, I had the opportunity to experience a professional working environment, receive direct guidance from mentors, and gain hands-on practice with real AWS services. This period helped me strengthen my foundational knowledge, expand new skills, and better understand the deployment processes for cloud computing solutions.\n1. Learning Process and Technical Development During my internship, I participated in researching and building an AI chatbot using Amazon Bedrock, Lambda, API Gateway, and S3, through which I:\nBasic AWS Knowledge:\nGained a stronger understanding of the basic structure of VPC, EC2, IAM, S3, and CloudFront Learned how to set up security groups and network ACLs Understood how to manage permissions and roles in IAM Practiced backup and restore operations for data on S3 Application Development:\nPracticed deploying applications using serverless architecture with AWS Lambda Learned how to design REST APIs using API Gateway Learned how to integrate AWS services together to create complete solutions Understood how to apply AI/LLM models through Amazon Bedrock Programming Skills:\nImproved Python/JavaScript programming skills in a more practical direction Learned to write clean code with clear structure Practiced debugging and troubleshooting when encountering errors Understood best practices when working with cloud services Through assigned tasks, I gained understanding of standard work processes, documentation writing, progress reporting, and coordination with mentors when facing technical challenges.\n2. Self-Assessment by Professional Criteria No. Criteria Description Good Fair Average 1 Technical Knowledge \u0026amp; Skills Applying AWS in practice, understanding and using basic to intermediate services ☐ ✅ ☐ 2 Learning Ability Self-research, quickly absorbing new knowledge ☐ ✅ ☐ 3 Initiative at Work Understanding requirements in advance, proposing solution approaches ✅ ☐ ☐ 4 Sense of Responsibility Completing work on time, ensuring quality ✅ ☐ ☐ 5 Discipline Following regulations, working according to proper procedures ✅ ☐ ☐ 6 Growth Mindset Always accepting feedback from mentors to improve skills ☐ ✅ ☐ 7 Communication - Work Reporting Clear progress reporting, discussing when encountering problems ☐ ✅ ☐ 8 Teamwork Ability Good cooperation in teams, supporting colleagues ✅ ☐ ☐ 9 Professional Attitude \u0026amp; Conduct Respecting colleagues, being serious and eager to learn ✅ ☐ ☐ 10 Problem-Solving Thinking Analyzing errors, finding root causes, providing solutions ☐ ✅ ☐ 11 Project Contribution Completing AI chatbot product as required, supporting processing logic improvements ✅ ☐ ☐ 12 Overall Assessment Level of meeting internship program requirements ✅ ☐ ☐ 3. Achievements Accomplished Main Project:\nSuccessfully built an AI chatbot capable of answering questions about AWS services Successfully deployed the application on AWS Lambda with API Gateway Integrated Amazon Bedrock so the chatbot can understand and respond in Vietnamese Designed a simple web interface to test the chatbot Professional Skills:\nGrasped basic DevOps processes: from development to deployment Understood monitoring and logging in cloud environments Learned how to optimize cost and performance for AWS services Gained practical experience with Infrastructure as Code (basic CloudFormation) Soft Skills:\nSignificantly improved presentation and product demo abilities Learned to work according to Agile methodology Understood the importance of documentation in real projects Developed troubleshooting skills when facing technical issues 4. Areas Needing Improvement Despite achieving significant progress, I recognize several areas that still need continued development:\nTechnical Aspects:\nEnhance ability to analyze and solve problems in more complex situations Strengthen knowledge of security and compliance in AWS Learn more deeply about advanced AWS services like EKS, ECS Improve skills in performance and cost optimization Soft Skills:\nImprove technical communication skills when presenting solutions to stakeholders Practice technical documentation writing according to enterprise standards Develop leadership skills when working in larger teams Improve ability to estimate effort and manage timelines for tasks 5. Conclusion My internship time at AWS Vietnam has significantly expanded my knowledge of cloud computing, especially AWS. I not only learned how to deploy cloud services but also learned how to work professionally in a modern technical environment.\nValue Gained:\nSolid AWS foundation for developing a career in cloud computing Understanding of real workflow in a tech company Portfolio project to demonstrate skills when applying for jobs Network with professionals in the industry I believe these experiences will be an important foundation for developing my career in Cloud Engineering - AI Engineering - System Architecture in the future.\n6. Acknowledgments I would like to express my sincere gratitude to:\nMentors and AWS Vietnam team who provided dedicated guidance and support throughout the internship The Company for creating opportunities to access advanced technologies and professional working environments The University for providing foundational knowledge that I could apply in practice This internship period was truly a turning point in my learning journey and professional development.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - S3 Static Website: + Introduction to S3 static hosting + Create S3 bucket for website + Load data and configure files + Enable static website feature + Configure public access block settings 10/13/2025 10/13/2025 https://000057.awsstudygroup.com/ 3 - Advanced S3 \u0026amp; CloudFront: + Configure public objects + Test website functionality + Accelerate static websites with CloudFront + Block all public access + Configure Amazon CloudFront distribution + Test Amazon CloudFront performance + Implement bucket versioning 10/14/2025 10/14/2025 https://000057.awsstudygroup.com/ 4 - S3 Management \u0026amp; Replication: + Move objects between locations + Configure replication object multi-region + Test cross-region replication + Clean up resources and environments + Review notes \u0026amp; best practices + Document lessons learned 10/15/2025 10/15/2025 https://000057.awsstudygroup.com/ 5 - AWS Security Hub Theory: + Learn about security standards + Understand AWS Security Hub fundamentals + Study compliance frameworks + Review security best practices + Prepare for hands-on implementation 10/16/2025 10/16/2025 https://000018.awsstudygroup.com/ 6 - Practice: + Enable Security Hub service + Configure security standards + Review score for each set of criteria + Analyze security findings + Clean up resources 10/17/2025 10/17/2025 https://000018.awsstudygroup.com/ Week 6 Achievements: Successfully implemented S3 Static Website hosting (Lab 57):\nCreated and configured S3 buckets for static website hosting Loaded website data and configured HTML/CSS files Enabled static website feature with proper settings Configured public access block policies for web access Tested website functionality and accessibility Mastered advanced S3 features and CloudFront integration:\nConfigured public objects for website content Accelerated static websites using Amazon CloudFront CDN Set up CloudFront distribution for global content delivery Tested CloudFront performance and caching behavior Implemented S3 bucket versioning for content management Gained expertise in S3 management and cross-region operations:\nMoved objects between different S3 locations Configured and tested multi-region replication Implemented cross-region data synchronization Performed comprehensive resource cleanup procedures Documented best practices and lessons learned Acquired foundational knowledge in AWS Security Hub:\nLearned about AWS security standards and compliance frameworks Understood Security Hub fundamentals and architecture Studied security best practices for AWS environments Prepared for hands-on security implementation Successfully implemented AWS Security Hub for security monitoring:\nEnabled Security Hub service in AWS account Configured security standards and compliance checks Reviewed and analyzed security scores for different criteria Analyzed security findings and recommendations Performed proper cleanup of security resources Developed comprehensive skills in AWS storage and security services combining practical implementation with theoretical understanding.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/5-workshop/5.6-cleanup/","title":"Clean Up","tags":[],"description":"","content":"CLEAN UP Clean up resources To avoid incurring future charges, you should unsubscribe from QuickSight if you do not plan to use it further.\nManage QuickSight:\nClick on your user icon in the top right corner. Choose Manage QuickSight. Account Settings:\nGo to Account settings. Choose Delete account. Type delete to confirm. Choose Delete account. Note: If you are on the Free Trial, you will not be charged until the trial ends, but it is good practice to clean up if you are done.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/7-feedback/","title":"Share and Feedback","tags":[],"description":"","content":" I would like to share my reflections on my internship at AWS Vietnam. I hope these notes are useful for the FCJ team and for future interns.\nOverall impression 1. Work environment The working environment at AWS Vietnam is professional and motivating. The modern office at Bitexco Financial Tower is an inspiring place to work. The workspace is tidy and comfortable, which helped me stay focused. The atmosphere is energetic without being overly stressful, making it easier to learn and grow.\n2. Mentor \u0026amp; admin support My mentor provided patient, clear guidance and encouraged questions. The admin team supported onboarding and documentation, which made it easier to get started. I appreciated being given chances to try solving problems on my own rather than being handed solutions.\n3. Relevance to my studies The tasks I worked on matched what I learned at university and also exposed me to new topics. This balance helped me strengthen foundational knowledge while gaining practical experience.\n4. Learning and skill development This internship offered strong opportunities to improve both technical and professional skills—teamwork, communication, and time management. Mr. Hoang Van Kha shared valuable practical advice that helped me shape my career direction.\n5. Culture and team spirit The company culture is positive: people are respectful, focused, and also friendly. When urgent work arises, teammates readily offer help, which made me feel included despite being an intern.\n6. Intern policies \u0026amp; benefits The internship provided a stipend and flexible hours when needed. Internal training sessions and workshops were particularly helpful for learning new skills.\nQuestions \u0026amp; feedback What did I enjoy most during the internship? - I most appreciated the team’s supportive attitude and professional approach. Whenever I encountered challenges, colleagues were willing to guide me, boosting my confidence and progress.\nWhat could be improved for future interns? - To help interns connect more effectively with the team and the company culture, I suggest:\n* Organizing more casual meetups or team events to help interns get to know each other and collaborate more easily.\n* Offering additional focused training sessions on specific technical topics to deepen interns’ expertise.\nWould I recommend this internship to friends? Why? - Yes. It’s a professional and welcoming environment with dedicated mentors and real opportunities to learn.\nSuggestions \u0026amp; closing thoughts Key suggestions: * Community building: Increase opportunities for in-person meetups so interns feel more connected to colleagues and company culture.\n* Knowledge sharing: Hold more in-depth internal sessions where mentors or teams share practical experience across domains.\nI would welcome the chance to continue working with the program or contribute to future projects—this internship has been an excellent environment for both technical and personal growth.\nFinal note: I sincerely thank the company and the FCJ team for their support and guidance during my internship. I hope these observations help improve the program for future cohorts.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - VPC \u0026amp; EC2 Setup: + Introduction to networking concepts + Create VPC (Virtual Private Cloud) + Create Security Group configurations + Launch EC2 instance in VPC + Configure incoming Web-hooks for Slack + Create tags for EC2 instance 10/20/2025 10/20/2025 https://000022.awsstudygroup.com/ 3 - Lambda Functions: + Create Role for Lambda function + Create Lambda Function for automation + Function stop instance implementation + Function start instance implementation + Check results and testing + Clean up resources 10/21/2025 10/21/2025 https://000022.awsstudygroup.com/ 4 - AWS Tags Management: + Introduction to AWS resource tagging + Using tags in AWS Console + Create EC2 Instance with tags + Managing tags in AWS Resources + Best practices for tagging strategy 10/22/2025 10/22/2025 https://000027.awsstudygroup.com/ 5 - Advanced Tags \u0026amp; Resource Groups: + Filter resources by tag + Using tags with AWS CLI + Create a Resource Group + Manage resources through Resource Groups + Clean up resources 10/23/2025 10/23/2025 https://000027.awsstudygroup.com/ 6 - IAM Policies \u0026amp; Role Management: + Introduction to IAM security + Create IAM user + Create IAM Policy + Create IAM Role + Switch Roles and check policy + Test EC2 access in Tokyo and North Virginia regions + Create EC2 instances with qualified tags + Edit resource tags on EC2 instances + Policy verification and cleanup resources 10/24/2025 10/24/2025 https://000028.awsstudygroup.com/ Week 7 Achievements: Successfully implemented AWS networking and compute infrastructure (Lab 22):\nCreated and configured VPC (Virtual Private Cloud) with proper network architecture Set up Security Groups with appropriate access rules Launched EC2 instances within VPC environment Configured incoming Web-hooks for Slack integration Applied proper tagging strategies to EC2 instances Mastered AWS Lambda serverless automation:\nCreated IAM roles for Lambda functions with appropriate permissions Developed Lambda functions for EC2 instance automation Implemented function to stop EC2 instances automatically Implemented function to start EC2 instances on demand Tested automation workflows and verified results Performed comprehensive resource cleanup procedures Gained expertise in AWS resource tagging and organization (Lab 27):\nUnderstood AWS resource tagging fundamentals and best practices Created EC2 instances with strategic tag implementation Managed tags across multiple AWS resources effectively Filtered and searched resources using tag-based queries Utilized AWS CLI for advanced tag management operations Developed proficiency with AWS Resource Groups:\nCreated Resource Groups for logical resource organization Managed collections of related AWS resources efficiently Implemented resource grouping strategies for operational efficiency Streamlined resource management through centralized grouping Mastered advanced IAM security and access control (Lab 28):\nCreated IAM users with specific permission requirements Developed custom IAM policies for granular access control Created and configured IAM roles for cross-service access Implemented role switching for different access scenarios Tested multi-region access control (Tokyo and North Virginia) Created EC2 instances with qualified tag requirements Edited and managed resource tags for policy compliance Verified policy effectiveness through comprehensive testing Acquired comprehensive skills in AWS security, automation, and resource management combining networking, compute services, serverless functions, and advanced access control mechanisms.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - IAM Restriction Policies: + Introduction to IAM security restrictions + Preparation and environment setup + Create restriction policy for limited access + Create IAM limited user with constraints + Test IAM user limits and restrictions + Clean up resources and policies 10/27/2025 10/27/2025 https://000030.awsstudygroup.com/ 3 - KMS \u0026amp; S3 Encryption: + Introduction to AWS encryption services + Create Policy and Role for KMS + Create Group and User for access control + Create Key Management Service (KMS) + Create Amazon S3 bucket + Upload data to S3 with encryption 10/28/2025 10/28/2025 https://000033.awsstudygroup.com/ 4 - CloudTrail \u0026amp; Athena: + Create AWS CloudTrail for logging + Configure logging to CloudTrail + Create Amazon Athena for analytics + Retrieve data with Athena queries + Test and share encrypted data on S3 + Resource cleanup and best practices 10/29/2025 10/29/2025 https://000033.awsstudygroup.com/ 5 - Advanced IAM Management: + Introduction about IAM fundamentals + Request to AWS service authentication + Authenticate requests process + Assume Role Process workflow + Create IAM Group with policies + Create IAM Users and assign groups + Check permissions and access control 10/30/2025 10/30/2025 https://000044.awsstudygroup.com/ 6 - Role Conditions \u0026amp; Restrictions: + Configure Role Condition policies + Create Admin IAM Role + Configure Switch role mechanism + Restrict role access with conditions + Limit switch role by IP address + Limit switch role by Time constraints + Clean up resources and policies 10/31/2025 10/31/2025 https://000044.awsstudygroup.com/ Week 8 Achievements: Mastered advanced IAM restriction policies and security constraints (Lab 30):\nImplemented IAM security restrictions for controlled access Set up secure environment with proper preparation procedures Created restriction policies for limited user access Developed IAM limited users with specific constraints Tested and validated IAM user limits and restrictions Performed comprehensive reso for data protection Created Amazon S3 buckets with advanced encryption features Uploaded and managed encrypted data inurce and policy cleanup Successfully implemented comprehensive AWS encryption and security services (Lab 33):\nMastered AWS encryption services including KMS implementation Created policies and roles specifically for KMS operations Established groups and users for proper access control Deployed Key Management Service (KMS) S3 environments Gained expertise in AWS logging, monitoring, and analytics:\nCreated and configured AWS CloudTrail for comprehensive logging Implemented logging mechanisms to CloudTrail for audit trails Deployed Amazon Athena for advanced data analytics Retrieved and analyzed data using Athena queries Tested and shared encrypted data on S3 with proper security Applied resource cleanup and security best practices Developed proficiency in advanced IAM management and authentication (Lab 44):\nUnderstood IAM fundamentals and service authentication processes Implemented AWS service request authentication mechanisms Mastered the assume role process workflow Created IAM groups with sophisticated policy attachments Developed IAM users and assigned appropriate group memberships Verified permissions and access control effectiveness Mastered advanced role conditions and security restrictions:\nConfigured advanced role condition policies Created Admin IAM roles with proper permissions Implemented switch role mechanisms for secure access Applied role access restrictions with conditional policies Limited switch role capabilities by IP address constraints Implemented time-based role access limitations Maintained security through proper resource cleanup Acquired comprehensive expertise in AWS security architecture combining IAM policies, encryption services, audit logging, and conditional access controls for enterprise-level security implementations.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 48 - EC2, S3 \u0026amp; IAM Integration: + Preparation and setup environment + Create EC2 Instance for testing + Create S3 bucket for storage + Generate IAM user and access key + Use access key for authentication + Create IAM role for EC2 + Using IAM role on EC2 + Clean up resources 11/03/2025 11/03/2025 https://000048.awsstudygroup.com/ 3 - Lab 05 - VPC \u0026amp; RDS Infrastructure: + Introduction to VPC and RDS concepts + Prerequisite steps and planning + Create a VPC for database environment + Create EC2 Security Group + Create RDS Security Group + Create DB Subnet Group 11/04/2025 11/04/2025 https://000005.awsstudygroup.com/ 4 - Lab 05 - EC2 \u0026amp; RDS Deployment: + Create EC2 instance in VPC + Create RDS database instance + Application deployment on EC2 + Database connectivity testing + Backup and restore procedures + Clean up resources and environment 11/05/2025 11/05/2025 https://000005.awsstudygroup.com/ 5 - Lab 43 - AWS Database Migration Service: + Getting started with DMS + Select your DMS source database + Select your DMS target database + Configure serverless replication + Monitor DMS migrations + Troubleshoot migrations with AWS DMS + Environment cleanup 11/06/2025 11/06/2025 https://000043.awsstudygroup.com/ 6 - Lab 35 - Data Analytics Pipeline: + Introduction to data analytics services + Create IAM Role and Policy + Create S3 Bucket for data storage + Create Kinesis Delivery Stream + Generate sample data + Create Glue Crawler for data catalog + Perform data transformation + Analysis with Athena + Visualize with QuickSight + Clean up resources 11/07/2025 11/07/2025 https://000035.awsstudygroup.com/ Week 9 Achievements: Successfully implemented comprehensive EC2, S3, and IAM integration (Lab 48):\nSet up complete AWS environment with proper preparation procedures Created and configured EC2 instances for testing and development Established S3 buckets for secure storage solutions Generated IAM users and access keys for authentication Implemented access key-based authentication mechanisms Created and configured IAM roles for EC2 instances Applied IAM roles to EC2 for secure service interactions Performed comprehensive resource cleanup and management Mastered VPC networking and RDS database infrastructure (Lab 05):\nGained deep understanding of VPC and RDS architectural concepts Executed prerequisite steps and infrastructure planning Created VPC environments optimized for database workloads Configured EC2 and RDS Security Groups with proper access rules Established DB Subnet Groups for multi-AZ deployments Deployed EC2 instances within VPC environments Created and configured RDS database instances Developed expertise in application deployment and database management:\nSuccessfully deployed applications on EC2 infrastructure Conducted comprehensive database connectivity testing Implemented backup and restore procedures for data protection Managed complete environment cleanup and resource optimization Acquired proficiency in AWS Database Migration Service (Lab 43):\nMastered DMS fundamentals and migration concepts Configured DMS source database connections Set up DMS target database environments Implemented serverless replication for scalable migrations Monitored DMS migration processes and performance Troubleshot complex migration issues with AWS DMS tools Applied proper environment cleanup procedures Successfully built end-to-end data analytics pipeline (Lab 35):\nUnderstood comprehensive data analytics service architecture Created IAM roles and policies for data services Established S3 buckets for scalable data storage Implemented Kinesis Delivery Streams for real-time data ingestion Generated and managed sample data for testing Created AWS Glue Crawlers for automated data catalog management Performed advanced data transformation processes Conducted sophisticated analysis using Amazon Athena Built interactive visualizations with Amazon QuickSight Maintained proper resource management and cleanup Acquired comprehensive expertise in AWS infrastructure, database services, migration tools, and data analytics platforms combining compute, storage, networking, database, and analytics services for enterprise-level solutions.\n"},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - DynamoDB Hands-on Labs: + Getting Started with DynamoDB + Create DynamoDB Tables and Load Sample Data + Explore DynamoDB with CLI (Read, Query, Scan, Insert/Update, Delete) + Work with Transactions and Global Secondary Indexes + Explore DynamoDB Console (View, Query, Scan, Modify Data) + Implement Backups (Point-In-Time Recovery, On-Demand, Scheduled) + Relational Modeling \u0026amp; Migration with DMS + Configure MySQL Environment and DynamoDB Migration + Clean up resources 11/10/2025 11/10/2025 https://000039.awsstudygroup.com/ 3 - LBED: Generative AI with DynamoDB zero-ETL to OpenSearch integration and Amazon Bedrock: + Getting Started - Obtain \u0026amp; Review Code + Service Configuration (OpenSearch Service Permissions, Enable Amazon Bedrock Models, Load DynamoDB Data) + Configure Integrations and Create zero-ETL Pipeline + Query and Conclusion - LADV: Advanced Design Patterns for Amazon DynamoDB: + Getting Started (Systems Manager Console, Python/AWS CLI, boto3, workshop files) + Exercise 1: DynamoDB Capacity Units and Partitioning + Exercise 2: Sequential and Parallel Table Scans + Exercise 3: Global Secondary Index Write Sharding + Exercise 4: Global Secondary Index Key Overloading + Exercise 5: Sparse Global Secondary Indexes + Exercise 6: Composite Keys + Exercise 7: Adjacency Lists + Exercise 8: DynamoDB Streams and AWS Lambda - LCDC: Change Data Capture for Amazon DynamoDB: + Getting Started (Cloud9, EC2 Instance) + Scenario Overview (Create DynamoDB Tables, Load Sample Data) + Change Data Capture using DynamoDB Streams + Change Data Capture using Kinesis Data Streams + Summary and Clean Up 11/11/2025 11/11/2025 https://000039.awsstudygroup.com/ 4 - LMR: Build and Deploy a Global Serverless Application with Amazon DynamoDB: + Getting Started + Module 1: Deploy the backend resources + Module 2: Explore Global Tables + Module 3: Interact with the Globalflix Interface + Global Tables Discussion Topics + Summary and Clean up - LEDA: Build a Serverless Event Driven Architecture with DynamoDB: + Getting Started and Overview + Optional - Pipeline Deep Dive + Lab 1: Connect the pipeline (StateLambda, MapLambda trigger, ReduceLambda) + Lab 2: Ensure fault tolerance and exactly once processing + Summary: Conclusions 11/12/2025 11/12/2025 https://000039.awsstudygroup.com/ 5 - LGME: Modeling Game Player Data with Amazon DynamoDB: + Getting Started + Plan your data model (Best Practices, Entity-Relationship Diagram, Access Patterns) + Core usage: user profiles and games (Design primary key, Retrieve item collections) + Create the table and bulk-load data + Find open games (Model sparse GSI, Create sparse GSI, Query and Scan sparse GSI) 11/13/2025 11/13/2025 https://000039.awsstudygroup.com/ 6 - LGME Continued: Advanced Game Operations: + Join and close games (Add users to game, Start a game) + View past games (Add inverted index, Retrieve games for user) + Summary \u0026amp; Cleanup - LDC: Design Challenges: + Retail Cart Scenario with References + Bank Payments Scenario with References + NoSQL Design: Reference Materials 10/14/2025 10/14/2025 https://000039.awsstudygroup.com/ Week 10 Achievements: Mastered comprehensive Amazon DynamoDB fundamentals and advanced concepts:\nNoSQL database principles and DynamoDB architecture Table creation, data modeling, and primary key design CLI and Console operations (CRUD, Query, Scan, Transactions) Backup strategies (Point-In-Time Recovery, On-Demand, Scheduled) Database migration from relational systems using DMS Implemented advanced DynamoDB design patterns and optimization techniques:\nGlobal Secondary Indexes (GSI) including sparse, write sharding, and key overloading Composite keys for complex querying patterns Adjacency lists for hierarchical data relationships Capacity units, partitioning, and performance optimization Sequential and parallel table scans Built real-time data processing and integration solutions:\nDynamoDB Streams for change data capture Kinesis Data Streams integration for high-volume processing Lambda functions for event-driven architectures Zero-ETL integration with OpenSearch and Amazon Bedrock for AI applications Developed serverless applications with global distribution:\nGlobal Tables for multi-region deployment Serverless event-driven architectures with fault tolerance Exactly-once processing and idempotency patterns Global serverless application deployment (Globalflix) Applied DynamoDB to real-world scenarios:\nGame player data modeling with matchmaking systems Retail cart and e-commerce data patterns Banking and financial services payment systems Advanced inverted indexes for complex queries Gained expertise in enterprise-level NoSQL design challenges:\nData modeling best practices for NoSQL systems Performance optimization and cost management Integration patterns with other AWS services Design challenges for retail and financial applications "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - AWS Glue \u0026amp; Amazon Athena: + Introducing AWS Glue \u0026amp; Amazon Athena + Preparation (Preparing the database, Building a database, Database Check) + Analysis of cost and usage performance (Data in Table, Cost, Tagging and Cost Allocation, Usage) + Clean up resources 11/17/2025 11/17/2025 https://000040.awsstudygroup.com/ 3 - Amazon DynamoDB Fundamentals: + Introduction (Core Components, Primary Key, Secondary Index, Naming Rules and Data Types, Read Consistency, Read/Write Capacity Mode) + Preparation and AWS Management Console Practice + Hands-on Operations (Create access key, Create table, Write/Read/Update data, Query data) + Advanced Features (Create Global Secondary Index, Query GSI) 11/18/2025 11/18/2025 https://000060.awsstudygroup.com/ 4 - DynamoDB Advanced Operations: + AWS CloudShell Operations (Create table, Write/Read/Update/Query data, Create and Query GSI) + AWS SDK Integration (Configure AWS CLI, Python and DynamoDB) + Comprehensive SDK Operations (Create/Write/Read/Update/Delete data, Load sample data, Query/Scan operations) + Resource Management (Delete table, Clean up resources) 11/19/2025 11/19/2025 https://000060.awsstudygroup.com/ 5 - AWS DataBrew Data Preparation: + Preparation (Creating Cloud9 Instance, Download Dataset, Check encoding, Upload Dataset to S3) + Data Preparation with DataBrew (Setting up DataBrew, Data Profiling) + Data Transformation (Clean \u0026amp; Transform Data, Preparing Next Table, Upload cleaned dataset) 11/20/2025 11/20/2025 https://000070.awsstudygroup.com/ 6 - Data Ingestion \u0026amp; Querying: + Data Ingestion with AWS Glue (Configuring roles, Creating Data Catalog, Transform to Parquet, Creating New Data Catalog, Schema verification) + Query with Athena (Install Athena, Basic query operations, Join 2 tables for complex analysis) 11/21/2025 11/21/2025 https://000070.awsstudygroup.com/ Week 11 Achievements: Mastered comprehensive AWS data analytics ecosystem and services:\nAWS Glue for ETL (Extract, Transform, Load) operations Amazon Athena for serverless interactive query analysis AWS DataBrew for visual data preparation and transformation DynamoDB for NoSQL database operations and management Integration patterns between analytics services Implemented complete data analytics workflows:\nDatabase preparation, building, and verification processes Cost analysis and optimization strategies for data services Tagging and cost allocation for resource management Performance analysis and usage pattern monitoring Gained expertise in Amazon DynamoDB fundamentals and advanced operations:\nCore components, primary keys, and secondary indexes Read consistency models and capacity mode configurations Console, CloudShell, and SDK-based operations Global Secondary Index creation and querying Comprehensive CRUD operations and data management Developed proficiency in data preparation and transformation:\nCloud9 development environment setup and management Dataset handling, encoding verification, and S3 integration DataBrew visual data profiling and quality assessment Data cleaning, transformation, and preparation workflows Multi-format data processing and optimization Built end-to-end data ingestion and querying capabilities:\nAWS Glue role configuration and Data Catalog management Data transformation to Parquet format for analytics optimization Schema verification and metadata management Athena setup and interactive querying operations Complex analytical queries including multi-table joins Applied enterprise-level data engineering best practices:\nCost-effective data storage and processing strategies Performance optimization for large-scale data operations Integration of multiple AWS services in cohesive workflows Resource cleanup and management for production environments "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Advanced Data Analytics \u0026amp; Visualization: + Advanced Athena Operations (Create table as selected, Create view, Data Partition, Columnar vs Row based analysis) + Visualization with QuickSight (Register for QuickSight, Configuring Permissions, Connect Dataset) + Dashboard Creation (Edit Dataset, Building Dashboard, Resource Cleanup) 11/24/2025 11/24/2025 https://000070.awsstudygroup.com/ 3 - AWS Glue \u0026amp; Kinesis Data Pipeline:** + Introduction and Preparatory steps + Ingest and Store (Creating Kinesis Firehose, Generate Dummy Data) + Catalog Data (Create IAM Role, Creating AWS Glue Crawlers, Verify tables in catalog) + Transform Data with AWS Glue (Interactive sessions, Glue Studio graphical interface) 11/25/2025 11/25/2025 https://000072.awsstudygroup.com/ 4 - Complete Analytics Pipeline:** + Data Transformation (AWS Glue DataBrew, Transform with EMR) + Analytics \u0026amp; Processing (Analysis with Athena, Kinesis Data Analytics) + Visualization \u0026amp; Serving (Visualize in QuickSight, Serve with Lambda) + Data Warehousing (Warehouse on Redshift, Clean up resources) 11/26/2025 11/26/2025 https://000072.awsstudygroup.com/ 5 - Advanced QuickSight Dashboard Development:** + Introduction and Preparation for professional dashboard creation + Build Dashboard (Update Dataset, Line chart, KPI and Insights, Pie chart, Create Pivot Table, Complete Dashboard) + Dashboard Improvements (Dashboard Format, Additional charts, Data Details Table) 11/27/2025 11/27/2025 https://000073.awsstudygroup.com/ 6 - Interactive Dashboard \u0026amp; Deployment:** + Create Interactive Dashboard (Create backup dashboard, Filter Settings, Filter Actions, Navigation Actions) + Dashboard Deployment (Publish dashboard, Clean up resources) 11/28/2025 11/28/2025 https://000073.awsstudygroup.com/ Week 12 Achievements: Mastered advanced data analytics and visualization capabilities:\nAdvanced Athena operations including table creation, views, and partitioning Columnar vs Row-based analysis for performance optimization QuickSight registration, permissions, and dataset configuration Professional dashboard creation and interactive visualization design Built comprehensive end-to-end data analytics pipelines:\nKinesis Firehose for real-time data ingestion and streaming AWS Glue Crawlers for automated data cataloging and schema discovery Multiple data transformation approaches (Glue Interactive, Glue Studio, DataBrew, EMR) Integration of streaming analytics with Kinesis Data Analytics Developed enterprise-level business intelligence solutions:\nMulti-service analytics architecture (Athena, QuickSight, Lambda, Redshift) Professional dashboard development with KPIs, charts, and pivot tables Interactive dashboard features including filters, actions, and navigation Production deployment with backup strategies and resource management Gained expertise in advanced QuickSight dashboard development:\nDataset optimization and preparation for visualization Multiple chart types (line charts, pie charts, KPIs, pivot tables) Dashboard formatting and professional presentation design Interactive features including filter settings and cross-visual actions Implemented production-ready data warehousing and serving solutions:\nAmazon Redshift for high-performance analytics and reporting AWS Lambda for serverless data serving and API endpoints Dashboard publishing and deployment workflows Comprehensive resource cleanup and cost management practices Applied advanced data engineering and analytics best practices:\nReal-time and batch processing pipeline architecture Data catalog management and metadata governance Performance optimization for large-scale analytics workloads Interactive business intelligence for data-driven decision making Enterprise deployment and maintenance strategies "},{"uri":"https://nt1510-l.github.io/aws-report.fcj/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://nt1510-l.github.io/aws-report.fcj/tags/","title":"Tags","tags":[],"description":"","content":""}]